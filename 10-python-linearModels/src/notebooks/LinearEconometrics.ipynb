{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Econometric Models\n",
    "\n",
    "So far in our Scientific Python adventures we have covered:\n",
    "\n",
    "* NumPy\n",
    "* SciPy - optimization\n",
    "* Pandas\n",
    "* Matplotlib (in your own time)\n",
    "\n",
    "For our final adventure into Python's Scientific stack we are going to look into estimating simple linear econometric models. This is just the tip of the iceberg as far as statistical modelling packages and capabilities in Python - but it is enough to get you hopefully confident enough to go looking for further modelling libraries on your own.\n",
    "\n",
    "This notebook is structured a little bit like how a simple project might look. We do all the tasks needed from downloading the data, some data visualization and the econometrics together. This should show you how far we have come on our Python Adventure. \n",
    "\n",
    "## Getting and Loading some data\n",
    "\n",
    "The plan is to use the data from AJR01's paper on linking institutions to ecconomic development as an example on how to run OLS and IV style regression models. That means we have to load the data.\n",
    "\n",
    "Let's use Python to download the file, and import it into our session as a Pandas DataFrame. Unfortunately Acemoglu's own data repository is weirdly formatted - so the work we are gonna do here is a little frustrating, but a good exercise in using what we may have learned last week with Julian.\n",
    "\n",
    "#### Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import requests\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://economics.mit.edu/files/5197'\n",
    "data_zip = '../data/table1.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the page\n",
    "response = requests.get(url, stream=True, verify=False)\n",
    "\n",
    "# save the zip to our preferred location\n",
    "with open(data_zip, 'wb') as out_file:\n",
    "    shutil.copyfileobj(response.raw, out_file)\n",
    "\n",
    "# clean up    \n",
    "del response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the data is located where we expected it to go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing into Pandas\n",
    "\n",
    "Unfortunately only `read_csv` comes with a built in option to read compressed data, so we have to do a bit of work here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "data_dir = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to extract the data from the zip file we just downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_ref = zipfile.ZipFile(data_zip, 'r')\n",
    "zip_ref.extractall(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set we want has the file ending `table1.dta`, so we need to get that value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = glob.glob(data_dir+'*table1.dta')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data with pandas `read_stata` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_stata(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "AJR's main hypothesis is to investigate the relationship between institutional differences and economic outcomes.\n",
    "\n",
    "To do this the paper operationalizes the variables of interest as follows:\n",
    "\n",
    "* economic outcomes are proxied by log GDP per capita in 1995, adjusted for exchange rates\n",
    "* institutional differences are proxied by an index of protection against expropriation on average over 1985-95, constructed by the Political Risk Services Group\n",
    "\n",
    "Before we jump on to running regressions, let's check out whether there is any relationship between these variables graphically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(x='avexpr', y='logpgp95', kind='scatter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there definitely appears to be a positive relationship. Next we could try and do a little more with this graph and plot a line through the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "\n",
    "# Dropping NA's is required to use numpy's polyfit\n",
    "df1_subset = data.dropna(subset=['logpgp95', 'avexpr'])\n",
    "\n",
    "# Use only 'base sample' for plotting purposes\n",
    "df1_subset = df1_subset[df1_subset['baseco'] == 1]\n",
    "\n",
    "X = df1_subset['avexpr']\n",
    "y = df1_subset['logpgp95']\n",
    "labels = df1_subset['shortnam']\n",
    "\n",
    "# Replace markers with country labels\n",
    "plt.scatter(X, y, marker='')\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    plt.annotate(label, (X.iloc[i], y.iloc[i]))\n",
    "\n",
    "# Fit a linear trend line\n",
    "plt.plot(np.unique(X),\n",
    "         np.poly1d(np.polyfit(X, y, 2))(np.unique(X)),\n",
    "         color='black')\n",
    "\n",
    "plt.xlim([3.3,10.5])\n",
    "plt.ylim([4,10.5])\n",
    "plt.xlabel('Average Expropriation Risk 1985-95')\n",
    "plt.ylabel('Log GDP per capita, PPP, 1995')\n",
    "plt.title('Relationship between expropriation risk and income')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Regression\n",
    "\n",
    "OK, so now we have visualized data let's run some OLS regressions. We will start with the simplest regression we can think of:\n",
    "\n",
    "$$\n",
    "logpgp95_i = \\beta_0 + \\beta_1 avexpr + u_i\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to load the `statsmodels` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels as sm\n",
    "sm.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's two ways to write out the specification for a regression, the easiest is by writing a formulaic description of the regression we want to estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_reg = smf.ols('logpgp95 ~ avexpr', data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate the regression specification, we use the `fit` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = simple_reg.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `results` object now contains a bunch of information. We can see what's inside (the names of) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the 'standard output' we have come to expect from software like STATA are contained in the `.summary()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An observation was mistakenly dropped from the results in the original paper (see the note located in maketable2.do from Acemogluâ€™s webpage), so our coefficients differ slightly from the ones in the original paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the predicted values from the regression using the `.predict()` attribute. Let's use these in a plot that has predicted versus actual values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing observations from whole sample\n",
    "\n",
    "df_plot = data.dropna(subset=['logpgp95', 'avexpr'])\n",
    "\n",
    "# Plot predicted values\n",
    "\n",
    "plt.scatter(df_plot['avexpr'], results.predict(), alpha=0.5, color='red', label='predicted')\n",
    "\n",
    "# Plot observed values\n",
    "plt.scatter(df_plot['avexpr'], df_plot['logpgp95'], alpha=0.5, label='observed')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Predicted vs actual values for simple OLS regression')\n",
    "plt.xlabel('avexpr')\n",
    "plt.ylabel('logpgp95')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mentioned that there is two ways to call a linear regression, and so far we showed how to use the formula interface which is probably the recommended way to do this in most applications.\n",
    "\n",
    "If we end up in a situation where we have to run a regression many times the formula interface actually slows down the code because python has to deconstruct it to get the exact information it needs. Examples where this may occur (and I have come across) are if a regression is part of a nested loop over structural parameters (BLP estimation for IO people), or if we are coding up our own indirect inference procedure.\n",
    "\n",
    "Here's another way to get the same result that is usually faster, although we have to import some additional functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "data = add_constant(data, has_constant='add')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.regression.linear_model import OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_reg2 = OLS(endog=data['logpgp95'], \n",
    "                     exog=data[['const', 'avexpr']], \n",
    "                     missing='drop')\n",
    "type(simple_reg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(simple_reg2.fit().summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this gives us exactly the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Regression\n",
    "\n",
    "Univariate regression is rarely enough in social science. At a bare minimum we are worried about other factors affecting GDP that are not included in our model that are also correlated with institutional quality. As reasonably good econometricians we know that this causes bias / inconsistency in our estimates for $\\hat{\\beta}$.\n",
    "\n",
    "So let's add more regressors! AJR argue that climate influences economic outcomes, and it is probably correlated to institutional quality (Discuss / think about why). They operationalize this by including latitude as a proxy. \n",
    "\n",
    "Notice that our data doesn't have that variable due to the 'interesting' way Acemoglu structures his data and do files on the web. We are going to have to pull another data set. We've created a simple function to make our lives easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import from another directory\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from lib import ajr\n",
    "\n",
    "weblink      = 'https://economics.mit.edu/files/5135'\n",
    "data_dir     = '../data/'\n",
    "zip_name     = 'table2.zip'\n",
    "table_number = 'table2'\n",
    "\n",
    "data = ajr.download_data(weblink, data_dir, zip_name, table_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the formula interface we can readily add this variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg2 = smf.ols('logpgp95 ~ avexpr + lat_abst', data=data)\n",
    "results2 = reg2.fit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is nice - but its a little hard to compare coefficients and the like across regressions that are far apart in the notebook. What we really want is a regression table that summarizes all of our results. `statsmodels` allows us to do this too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.iolib.summary2 import summary_col\n",
    "\n",
    "info_dict={'R-squared' : lambda x: \"{:.2f}\".format(x.rsquared),\n",
    "           'No. observations' : lambda x: \"{0:d}\".format(int(x.nobs))}\n",
    "\n",
    "results_table = summary_col(results=[results,results2],\n",
    "                            float_format='%0.2f',\n",
    "                            stars = True,\n",
    "                            model_names=['Model 1',\n",
    "                                         'Model 2'],\n",
    "                            info_dict=info_dict,\n",
    "                            regressor_order=['avexpr',\n",
    "                                             'lat_abst',\n",
    "                                             'const'])\n",
    "\n",
    "results_table.add_title('Table - OLS Regressions')\n",
    "\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we decide we want this table as latex output we can do that too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../out/tables/ols_summary.tex', 'w') as file_handle:\n",
    "    file_handle.write(results_table.as_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "It's your turn!\n",
    "\n",
    "1. Add the following variables to the regression using the formula interface:\n",
    "    * 'asia', 'africa', 'other'\n",
    "2. Add the same variables as (1); but use the alternative interface that requires you to specify 'endog' and 'exog' variables\n",
    "3. Export the results to a latex file 'out/tables/ols-regressions'. Give the columns meaningful names, and make sure you don't have stars in your output to satisfy the new AER policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robustifying Standard Errors\n",
    "\n",
    "So far our regressions have imposed heteroskedastic standard errors.\n",
    "\n",
    "A simple test for Heteroskedasticity is the Breusch Pagan test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg3 = smf.ols('logpgp95 ~ avexpr + lat_abst + asia + africa + other', data=data)\n",
    "results3 = reg3.fit() \n",
    "\n",
    "from statsmodels.compat import lzip\n",
    "import statsmodels.stats.api as sms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ['Lagrange multiplier statistic', 'p-value', \n",
    "        'f-value', 'f p-value']\n",
    "test = sms.het_breushpagan(results3.resid, results3.model.exog)\n",
    "lzip(name, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some evidence for heteroskedasticity. Let's robustify standard errors. Statsmodels has 'HC0' 'HC1', 'HC2' and 'HC3' standard errors. The STATA default is HC1, so for better or worse let's geta a summary with those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_ols = reg3.fit(cov_type='HC1')\n",
    "print(robust_ols.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also options for clustered standard errors. A nice summary of what is available albiet on a different data set is available [here](http://www.vincentgregoire.com/standard-errors-in-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrumental Variables Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AJR discuss that the OLS regression results may suffer from endogeneity due to:\n",
    "\n",
    "\n",
    "* richer countries may be able to afford or prefer better institutions\n",
    "* variables that affect income may also be correlated with institutional differences\n",
    "* the construction of the index may be biased; analysts may be biased towards seeing countries with higher income having better institutions\n",
    "\n",
    "They propose a 2SLS solution using settler mortality to instrument for institutional differnences: They hypothesize that higher mortality rates of colonizers led to the establishment of institutions that were more extractive in nature (less protection against expropriation), and these institutions still persist today.\n",
    "\n",
    "A scatter plot provides some evidence towards the necessary condition of a correlation between the instrument and the endogenous variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weblink      = 'https://economics.mit.edu/files/5136'\n",
    "data_dir     = '../data/'\n",
    "zip_name     = 'table4.zip'\n",
    "table_number = 'table4'\n",
    "\n",
    "data = ajr.download_data(weblink, data_dir, zip_name, table_number)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping NA's is required to use numpy's polyfit\n",
    "df1 = data.dropna(subset=['logem4', 'avexpr'])\n",
    "\n",
    "X = df1['logem4']\n",
    "y = df1['avexpr']\n",
    "labels = df1['shortnam']\n",
    "\n",
    "# Replace markers with country labels\n",
    "plt.scatter(X, y, marker='')\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    plt.annotate(label, (X.iloc[i], y.iloc[i]))\n",
    "\n",
    "# Fit a linear trend line\n",
    "plt.plot(np.unique(X),\n",
    "         np.poly1d(np.polyfit(X, y, 1))(np.unique(X)),\n",
    "         color='blue')\n",
    "\n",
    "plt.xlim([1.8,8.4])\n",
    "plt.ylim([3.3,10.4])\n",
    "plt.xlabel('Log of Settler Mortality')\n",
    "plt.ylabel('Average Expropriation Risk 1985-95')\n",
    "plt.title('First-stage relationship between settler mortality and expropriation risk')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's estimate the first stage relationship for the univariate regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.query('baseco ==1')\n",
    "\n",
    "first_state_reg = smf.ols('avexpr ~ logem4', \n",
    "                          data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_state_reg.fit(cov_type='HC1').summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first stage seems OK with a t-state > 3 (approx F > 10). Let's do an IV2SLS estimation using the `linearmodels` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linearmodels.iv import IV2SLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv_regression = IV2SLS.from_formula('logpgp95 ~ 1 + [avexpr ~ logem4]', \n",
    "                                      data=data).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv_regression.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Challenge\n",
    "\n",
    "1. Estimate the IV model for the regression that includes 'asia', 'africa' and 'other' as exogenous explanatory variables\n",
    "2. Produce a regression table that compares the 4 coefficient estimates on 'avexpr', all using robust standard errors.\n",
    "3. Use a robust Hausman test to conduct a formal test for endogeneity. Was the IV strategy appropriate based on this test result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
